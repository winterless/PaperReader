# Seed Papers Download Report

Downloaded into `inbox/pdf_raw/seed_papers/`.

| Domain | Title | Source Page | PDF Link | Local File | Status | Size (MB) | Notes |
|---|---|---|---|---|---|---:|---|
| training_infra | Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism | https://arxiv.org/abs/1909.08053 | https://arxiv.org/pdf/1909.08053.pdf | `inbox/pdf_raw/seed_papers/training_infra__megatron_lm_2019.pdf` | ok | 3.67 |  |
| training_infra | ZeRO: Memory Optimizations Toward Training Trillion Parameter Models | https://arxiv.org/abs/1910.02054 | https://arxiv.org/pdf/1910.02054.pdf | `inbox/pdf_raw/seed_papers/training_infra__zero_2020.pdf` | ok | 0.74 |  |
| training_infra | DeepSeek-V3 Technical Report | https://arxiv.org/abs/2412.19437 | https://arxiv.org/pdf/2412.19437.pdf | `inbox/pdf_raw/seed_papers/training_infra__deepseek_v3_technical_report_2024.pdf` | ok | 1.80 |  |
| alignment | Training language models to follow instructions with human feedback | https://arxiv.org/abs/2203.02155 | https://arxiv.org/pdf/2203.02155.pdf | `inbox/pdf_raw/seed_papers/alignment__instructgpt_2022.pdf` | ok | 1.71 |  |
| alignment | Direct Preference Optimization: Your Language Model is Secretly a Reward Model | https://arxiv.org/abs/2305.18290 | https://arxiv.org/pdf/2305.18290.pdf | `inbox/pdf_raw/seed_papers/alignment__dpo_2023.pdf` | ok | 1.24 |  |
| alignment | Don't Stop Pretraining: Adapt Language Models to Domains and Tasks | https://arxiv.org/abs/2004.10964 | https://arxiv.org/pdf/2004.10964.pdf | `inbox/pdf_raw/seed_papers/alignment__dont_stop_pretraining_2020.pdf` | ok | 1.68 |  |
| agent | ReAct: Synergizing Reasoning and Acting in Language Models | https://arxiv.org/abs/2210.03629 | https://arxiv.org/pdf/2210.03629.pdf | `inbox/pdf_raw/seed_papers/agent__react_2022.pdf` | ok | 0.60 |  |
| agent | Toolformer: Language Models Can Teach Themselves to Use Tools | https://arxiv.org/abs/2302.04761 | https://arxiv.org/pdf/2302.04761.pdf | `inbox/pdf_raw/seed_papers/agent__toolformer_2023.pdf` | ok | 0.63 |  |
| foundation | Attention Is All You Need | https://arxiv.org/abs/1706.03762 | https://arxiv.org/pdf/1706.03762.pdf | `inbox/pdf_raw/seed_papers/foundation__attention_is_all_you_need_2017.pdf` | ok | 2.11 |  |
| foundation | Scaling Laws for Neural Language Models | https://arxiv.org/abs/2001.08361 | https://arxiv.org/pdf/2001.08361.pdf | `inbox/pdf_raw/seed_papers/foundation__scaling_laws_2020.pdf` | ok | 2.38 |  |
| foundation | Training Compute-Optimal Large Language Models | https://arxiv.org/abs/2203.15556 | https://arxiv.org/pdf/2203.15556.pdf | `inbox/pdf_raw/seed_papers/foundation__chinchilla_2022.pdf` | ok | 5.73 |  |
